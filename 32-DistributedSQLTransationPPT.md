
## 分布式SQL数据库

2017年5月
note: 
制作ppt：https://github.com/hakimel/reveal.js
文档：http://192.168.100.2:81/distributedSQLDB/docs
https://github.com/xingjianwei/docs
---
## 关系型数据库现状 
![db-engines](images/db-engines.png)
note:
https://db-engines.com/en/ranking
Oracle，MySQL 和 Microsoft SQL Server 也依然稳居前三名。不过位列第一、二名的 Oracle 和 MySQL 本月得分分别暴跌 47.68 分和 24.59 分，包揽了跌幅榜的冠、亚军。由于 Oracle 跌幅更大，目前两者仅相差 14.28 分。
--
## 关系型数据库现状
![db-engines](images/db-engines-2.png)
--
## 关系型数据库现状
DB-Engines 排名的数据依据 5 个不同的因素：
* Google以及Bing搜索引擎的关键字搜索数量
* Google Trends的搜索数量
* Indeed网站中的职位搜索量
* LinkedIn中提到关键字的个人资料数
* Stackoverflow上相关的问题和关注者数量
--
## MySQL & MariaDB

## PostgreSQL
---
## NOSQL回顾
--
### NOSQL的优点
* 易扩展
* 大数据量，高性能
* 灵活的数据模型
* 高可用
note:
* 易扩展
NoSQL数据库种类繁多，但是有一个共同的特点，都是去掉了关系型数据库的关系型特性。数据之间无关系，这样就非常容易扩展。也无形之间，在架构的层面上带来了可扩展的能力。
* 大数据量，高性能
NoSQL数据库都具有非常高的读写性能，尤其在大数据量下，同样表现优秀。这得益于它的无关系性，数据库的结构简单。一般MySQL使用Query Cache，每次表更新Cache就失效，是一种大粒度的Cache，针对web2.0的交互频繁的应用，Cache性能不高。而NoSQL的Cache是记录级的，是一种细粒度的Cache，所以NoSQL在这个层面上来说性能就要高很多了。
* 灵活的数据模型
NoSQL无需事先为要存储的数据建立字段，随时可以存储自定义的数据格式。而在关系型数据库里，增删字段是一件非常麻烦的事情。如果是非常大数据量的表，增加字段简直就是一个噩梦。这点在大数据量的web2.0时代尤其明显。
* 高可用
NoSQL在不太影响性能的情况下，就可以方便地实现高可用的架构。比如Cassandra、HBase模型，通过复制模型也能实现高可用。
--
### NOSQL的缺点
* 没有标准
* 不支持存储过程
* 不支持SQL
* 支持的特性不够丰富，产品不够成熟
note:
* 没有标准
没有对NoSQL数据库定义的标准，所以没有两个NoSQL数据库是平等的。
* 不支持存储过程
NoSQL数据库中大多没有存储过程。
* 不支持SQL
NoSQL大多不提供对SQL的支持：如果不支持SQL这样的工业标准，将会对用户产生一定的学习和应用迁移上的成本。
* 支持的特性不够丰富，产品不够成熟
现有产品所提供的功能都比较有限，不像MS SQL Server和Oracle那样能提供各种附加功能，比如BI和报表等。大多数产品都还处于初创期，和关系型数据库几十年的完善不可同日而语。
--
### NoSQL与SQL的对比 
|      | RDBMS        | NoSQL     |
| ---- | ------------ | --------- |
| 模式   | 预定义的模式       | 没有预定义的模式  |
| 查询语言 | 结构化查询语言（SQL） | 没有声明性查询语言 |
| 一致性  | 严格的一致性       | 最终一致性     |
| 事务   | 支持           | 不支持       |
| 理论基础 | ACID         | CAP, BASE |
| 扩展   | 纵向扩展         | 横向扩展(分布式) |
---
## NoSQL数据库的分类

--
### 键值(Key-Value)存储数据库
* Redis：Redis是一个高性能的key-value存储系统，和Memcached类似，它支持存储的value类型相对更多。
* LevelDB：Leveldb是Google开发的一个非常高效的kv数据库，支持billion级别的数据量，在这个数量级别下还有着非常高的性能，主要归功于它的良好的设计，特别是LSM算法。
* RocksDB：RocksDB 是一个来自 facebook 的可嵌入式的支持持久化的 key-value 存储系统，RocksDB 基于 LevelDB 构建。
note:
这一类数据库主要会使用到哈希表，在这个表中有一个特定的键和一个指针指向特定的数据。Key/value模型对于IT系统来说优势在于简单、易部署。但是如果DBA只对部分值进行查询或更新的时候，Key/value就显得效率低下了。
--
### 列存储数据库 
* Cassandra：Apache Cassandra 是一套开源分布式 Key-Value 存储系统。它最初由 Facebook 开发，类似于 Google 的 BigTable。
* HBase：HBase在Hadoop之上提供了类似于Bigtable的能力，是一个适合于非结构化数据存储的数据库，HBase基于列的而不是基于行的模式。
note:
这部分数据库通常是用来应对分布式存储的海量数据。键仍然存在，但是它们的特点是指向了多个列。这些列是由列族来安排的。
--
### 文档型数据库 
* MongoDB：MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。MongoDB 将数据存储为一个文档，数据结构由键值(key=>value)对组成。MongoDB 文档类似于 JSON 对象。
* CouchDB：MongoDB与CouchDB很相似，都是文档型存储，数据存储格式都是JSON型。CouchDB是一个MVCC（支持多版本控制）的系统，而MongoDB是一个update-in-place的系统。
* SequoiaDB：巨杉数据库是一款支持SQL、高并发、实时性、分布式、可扩展、灵活存储的操作型?NewSQL?数据库。
note:
文档型数据库的灵感来自于Lotus Notes办公软件，它同第一种键值存储相类似。该类型的数据模型是版本化的文档，半结构化的文档以特定的格式存储，比如JSON。文档型数据库可以看作是键值数据库的升级版，允许之间嵌套键值。而且文档型数据库比键值数据库的查询效率更高。
--
### 图形(Graph)数据库
* Neo4J：Neo4j是图数据库中一个主要代表，用Java实现。社区版免费开源，只能在本机嵌入式使用；企业版闭源，集群也只是HA高可用，不能进行分布式存储。图数据结构导致写入性能差。
* Titan：Titan 是一个分布式的图形数据库，特别为存储和处理大规模图形而优化。Titan插件式特性让它搭建在一些成熟的数据存储层上（ Apache Cassandra、Apache HBase、 Oracle BerkeleyDB）；插件式索引架构可以整合 ElasticSearch 和Lucene技术；内置实现 Blueprints  graph API，支持 TinkerPop所有的技术。
* Spark GraphX：基于Spark平台提供对图计算和图挖掘简洁易用的而丰富的接口，极大的方便了对分布式图处理的需求。 
--
### 图形(Graph)数据库
* ArangoDB：ArangoDB支持灵活的数据模型，比如文档Document、图Graph以及键值对Key-Value存储。
* OrientDB：OrientDB是兼具文档数据库的灵活性和图形数据库管理链接能力的可深层次扩展的文档-图形数据库管理系统。
note:
图形结构的数据库同其它行列以及刚性结构的SQL数据库不同，它是使用灵活的图形模型，并且能够扩展到多个服务器上。NoSQL数据库没有标准的查询语言(SQL)，因此进行数据库查询需要制定数据模型。许多NoSQL数据库都有REST式的数据接口或者查询API。
--
## ArangoDB、MongoDB和Neo4j性能比较
![graphsql](images/graphdb.png)
note:
测试所用的数据集是一个社交网络快照，由斯坦福大学的SNAP提供，其中包含160多万个顶点（代表个人资料）和3000多万条边（代表朋友关系）。他们用顶点数据做文档数据库测试，用顶点和边的综合数据做图数据库测试。测试场景如下：

单次读：单文档（个人资料）读取（10万次）；
单次写：单文档写入（10万次）；
聚合：计算社交网络的年龄分布，即每个年龄出现多少次；
相邻顶点：为500个顶点查找直接相邻顶点以及相邻顶点的相邻顶点；
最短路径：在一个高度连通的社交图中查找19条最短路径。
其中，所有数据库都执行相同的操作，所有测试用例都是用JavaScript实现，在node.js中运行。
---
## 新一代分布式SQL数据库(NewSQL)
---
## NewSQL的必备特点 TiDB
* 无缝的水平扩展，应用层可以不用关心存储的容量和吞吐。
* SQL 支持，SQL 有着良好的易用性和生态系统。业务层如果已经在使用 SQL ，转变到 NoSQL 上是个极其痛苦的过程。
* 完整的 ACID 事务支持，应用层不需要关心跨行事务的实现，不需要写很多 hack 代码来确保数据的安全性。
* 更强的 MVCC（多版本并发控制），可以无锁的访问任意时间点的数据库快照。
* 在线的 Schema 变更，不需要每次更新Schema都要面临停止服务的窘境。
* 零迁移成本，兼容现有的协议或者查询语法，已有的代码不需要做改动或很少改动自动就能获得扩展的能力。
---
### Spanner
* 完整的 SQL 支持，ACID 事务；
* 弹性伸缩能力；
* 自动的故障转移和故障恢复，多机房异地灾备。
note:
Spanner 的创新之处在于通过硬件（GPS时钟+原子钟）来解决时钟同步的问题。
Replica 这层使用 Paxos 复制。
F1 构建在 Spanner 之上，对外提供了更丰富的 SQL 语法支持，F1 更像一个分布式 MPP SQL——F1 本身并不存储数据，而是将客户端的 SQL 翻译成类似 MapReduce 的任务，调用 Spanner 来完成请求。
---
### TiDB
- TiDB开源的分布式数据库，参考 Google F1/Spanner 实现了水平伸缩，一致性的分布式事务，多副本同步复制等重要 NewSQL 特性。
---
### CockroachDB
- CockroachDB是一个基于事务和强一致性键值存储构建的分布式SQL数据库。 它支持水平缩放; 可以容忍磁盘、机器、机架，甚至数据中心故障，并能在极短时间内无需人工干预的恢复服务; 支持强一致的ACID事务; 并提供了类SQL API来构造、操作和查询数据。
---
### OceanBase
- OceanBase是阿里巴巴自主研发的一个支持海量数据的高性能分布式数据库系统。
- OceanBase使用了分布式技术和无共享架构，来自业务的访问会自动分散到多台数据库主机上。
---
### Kudu
- Kudu是Todd Lipcon@Cloudera带头开发的存储系统，其整体应用模式和Hbase比较接近，即支持行级别的随机读写，并支持批量顺序检索功能。
- Kudu定位于应对快速变化数据的快速分析型数据仓库，希望靠系统自身能力，支撑起同时需要高吞吐率的顺序和随机读写的应用场景（可能的场景，比如时间序列数据分析，日志数据实时监控分析），提供一个介于HDFS和HBase的性能特点之间的一个系统，在随机读写和批量扫描之间找到一个平衡点，并保障稳定可预测的响应延迟。
--
### Kudu的定位
![kudu-pos](images/kudu-pos.png)
--
### 没有使用kudu之前，小米的架构
![kudu-case-pre](images/kudu-case-pre.png)
--
### 使用了Kudu以后，小米的架构
![kudu-case-after](images/kudu-case-after.png)
---
### SequoiaDB
* Share-Nothing的分布式数据库
* 同时具备高性能与高可用的特性
* 标准SQL支持
* 分布式架构
* 对象存储
* JSON与块存储双引擎
* 集成Spark内存计算框架
---
### BDRT
- 天云大数据BDRT（Beagledata Realtime Transaction）是一款大规模高并发支持灵活查询的实时查询引擎，具有高可用、可横向扩展、健壮性的特点，支持数据自动均匀分布、支持索引及事务控制、支持REST、SQL、SDK等接口，支持上千个用户并发的进行实时查询。
--
### BDRT查询引擎具有下列特性
* 与hadoop生态圈紧密结合，可与其他hadoop组件进行无缝集成。
* 支持数据和用户的高扩展，水平扩展非常容易。
* 具有容错性的数据分发和备份，对索引分片，并对每个分片创建多个副本。每个副本都可以对外提供服务。一个副本的异常不会对整个集群提供索引服务造成影响。
* 支持高可用性和热备份。
* 支持对数据进行各种高级查询，包括交集、联集、排除、通配符、范围、分页、排序、Group等。
* 读写严格一致，支持ACID和最终一致性。支持事务的提交和回滚，有效保障了数据的完整性。
* 数据查询的秒级毫秒级响应，从而支持OLTP。
--
### 适用场景
* 利用BDRT低延时、高性能、海量存储等特性，满足需要从海量的历史和实时数据中秒级获取有效信息的场景。
* 在分布式背景下，数据量不断的增长，需要高速的读写，并有复杂的ETL需要的场景。
* 用户使用频率非常高，重要程度仅次于核心应用，对数据的丢失以及服务的中断零容忍的场景。
* 对数据的一致性有要求的场景。
---
## 开源SQL引擎介绍
- SQL 引擎独立于数据存储系统（相对而言，关系型数据库将查询引擎和存储绑定到一个单独的紧耦合系统中），提供了更大的灵活性，尽管存在潜在的性能损失。
---
### Apache Hive
- Apache Hive 是Hadoop 生态系统中的第一个SQL 框架。Facebook 的工程师在2007年介绍了Hive，并在2008年将代码捐献给Apache 软件基金会。2010年9月，Hive 毕业成为Apache 顶级项目。

- 使用Apache Tez，和ORCfile，对Hive 的查询产生了明显的提速。
---
### Apache Impala
- 2012年，Cloudera 推出了Impala，一个开源的MPP SQL 引擎，作为Hive 的高性能替代品。Impala 使用HDFS 和HBase，并利用了Hive 元数据。但是，它绕开了使用MapReduce 运行查询。
---
### Spark SQL
- Spark SQL 用户可以运行SQL 查询，从Hive 中读取数据，或者使用它来创建Spark Dataset和DataFrame。Spark SQL 的接口向Spark 提供了数据结构和执行操作的信息，Spark 的Catalyst 优化器使用这些信息来构造一个高效的查询。
---
### Apache Drill
- MapR 领导的一个团队，构建的一个Google Dremel 的开源版本，一个交互式的分布式热点分析系统。
---
### Apache HAWQ
- Pivotal 软件在2012年推出了一款商业许可的高性能SQL 引擎，并于2015年9月进入了Apache 孵化器程序。

- HAWQ 的一个特点是它支持Apache MADlib，一个同样在孵化器中的SQL 机器学习项目。
---
### Presto
- Facebook 工程师在2012年发起了Presto 项目，作为Hive 的一个快速交互的取代。在2013年推出时，成功的支持了超过1000个Facebook 用户和每天超过30000个PB级数据的查询。2013年Facebook 开源了Presto。

- Presto 是一个非常活跃的项目，有一个巨大的和充满活力的贡献者社区。该团队发布的速度非常快--2016年共发布了42个版本。
---
### Apache Calcite
* SQL 解析器、验证器和JDBC 驱动
* 查询优化工具，包括关系代数API，基于规则的计划器和基于成本的查询优化器
* Apache Hive 使用Calcite 进行基于成本的查询优化，而Apache Drill 和Apache Kylin 使用SQL 解析器。
---
### Apache Kylin
- 是一个具有SQL 接口的OLAP 引擎。由eBay 开发并捐献给Apache，Kylin于2014年10月在github开源，并在2014年11月加入Apache孵化器，Kylin 在2015年毕业成为顶级项目。

- 2016年3月，Apache Kylin核心开发成员创建成立的创业公司Kyligence 提供商业支持的数据仓库产品KAP 
---
### Apache Phoenix
- Apache Phoenix 是一个运行在HBase 上的SQL 框架，绕过了MapReduce。Salesforce 开发了该软件并在2013年捐献给了Apache。2014年5月项目毕业成为顶级项目。Hortonworks 数据平台中包含该项目。自从领先的SQL 引擎都适配HBase 之后，Phoenix的重要性大大下降。
---
## 基础技术原理和名称术语
---
## CAP 
* 一致性（C）：在分布式系统中的所有数据备份，在同一时刻是否同样的值。（等同于所有节点访问同一份最新的数据副本）
* 可用性（A）：在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。（对数据更新具备高可用性）
* 分区容错性（P）：以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。
note:
在分布式环境下面，P 是铁定存在的，也就是只要我们有多台机器，那么网络隔离分区就一定不可避免，所以在设计系统的时候我们就要选择到底是设计的是 AP 系统还是 CP 系统。实际环境中，主要表现在：

1. 网络分区出现的概率很低，所以我们没必要去刻意去忽略 C 或者 A。多数时候，应该是一个 CA 系统。
2. CAP 里面的 A 是 100% 的可用性，但实际上，我们只需要提供 high availability，也就是仅仅需要满足 99.99% 或者 99.999% 等几个 9 就可以了。
---
## ACID
- CAP并不适合再作为一个适应任何场景的定理，它的正确性更加适合基于原子读写的NoSQL场景。
--
### 原子性 (Atomic)
- 一个事务包含多个操作，这些操作要么全部执行，要么全都不执行。实现事务的原子性，要支持回滚操作，在某个操作失败后，回滚到事务执行之前的状态。
--
### 一致性(Consistency) 
- 一致性是指事务使得系统从一个一致的状态转换到另一个一致状态。在事务开始和完成时，数据必须保持一致状态，相关的数据规则必须应用于事务的修改，以保证数据的完整性，事务结束时，所有的内部数据结构必须正确。
note:
*强一致性：**读操作可以立即读到提交的更新操作。
*弱一致性：**提交的更新操作，不一定立即会被读操作读到，此种情况会存在一个不一致窗口，指的是读操作可以读到最新值的一段时间。
*最终一致性：**是弱一致性的特例。事务更新一份数据，最终一致性保证在没有其他事务更新同样的值的话，最终所有的事务都会读到之前事务更新的最新值。如果没有错误发生，不一致窗口的大小依赖于：通信延迟，系统负载等。

其他一致性变体还有：
*单调一致性：**如果一个进程已经读到一个值，那么后续不会读到更早的值。
*会话一致性：**保证客户端和服务器交互的会话过程中，读操作可以读到更新操作后的最新值。
--
### 隔离性(Isolation) 
- 在数据库操作中，为了有效保证并发读取数据的正确性，提出的事务隔离级别。他表示并发事务之间互相影响的程度，比如一个事务会不会读取到另一个未提交的事务修改的数据。
--
### 持久性(Durability)
- 事务提交后，对系统的影响是永久的，即使系统出现故障也能够保持。
---
## 权衡一致性与可用性 - BASE理论
- 由eBay架构师DanPritchett提出，Base是对CAP中一致性A和可用性C权衡的结果，源于提出者自己在大规模分布式系统上实践的总结。

- 核心思想是无法做到强一致性，但每个应用都可以根据自身的特点，采用适当方式达到最终一致性。
--
### BA - Basically Available - 基本可用
- Base = Basically Available + Soft state + EventuallyConsistent 基本可用性+软状态+最终一致性
- 是指分布式系统在出现故障的时候，允许损失部分可用性，即保证核心功能或者当前最重要功能可用。对于用户来说，他们当前最关注的功能或者最常用的功能的可用性将会获得保证，但是其他功能会被削弱。
--
### S – Soft State - 软状态
- 允许系统数据存在中间状态，但不会影响到系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步时存在延时。
--
### E - Eventually Consistent - 最终一致性

* 因果一致性
* 读己之所写(因果一致性特例)
* 会话一致性
* 单调读一致性
* 单调写一致性
---
## 分布式存储算法和技术实现
---
## 分布式系统的定义
- 一个分布式系统由多个计算节点组成，每个节点承担一定的计算和存储，节点之间通过网络连接到一起，协同工作形成一个整体。
---
## 分布式系统中的一致性问题
---
### 一致性问题简介

* 消息传递：两军问题(two-army problem)。现实网络不是可靠的，存在消息丢失、消息延时、传递消息顺序错乱等问题。
* 节点问题：节点宕机或者节点宕机后恢复，都可能导致一致性问题。
* 网络分化：网络链路有问题，将节点分割成几个部分。
* 协议（共识）：拜占庭将军（Byzantine failure）问题。
--
### 两军问题(two-army problem)
-两军问题(two-army problem)是一个典型的信道安全问题。即：白军驻扎在沟渠里，蓝军则分散在沟渠两边。白军比任何一支蓝军都更为强大，但是蓝军若能同时合力进攻则能够打败白军。他们不能够远程的沟通，只能派遣通信兵穿过沟渠去通知对方蓝军协商进攻时间。是否存在一个能使蓝军必胜的通信协议，这就是两军问题。
--
### 拜占庭将军（Byzantine failure）问题
- 拜占庭将军（Byzantine failure）问题，是由莱斯利·兰伯特提出的点对点通信中的基本问题。含义是在存在消息
丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。因此对一致性的研究一般假设信道是可靠的
，或不存在本问题。
---
### 一致性基本概念
--
#### FLP定理
- FLP定理(FLP impossibility)已经证明在一个收窄的模型中(异步环境并只存在节点宕机)，不能同时满足强一致和可用性。强一致：它要求所有节点状态一致、共进退；可用：它要求分布式系统24*7无间断对外服务。工程实践上根据具体的业务场景，或保证强一致，或在节点宕机、网络分化的时候保证可用。总要在一致性和可用上做一定的取舍。
--
#### XA
- XA 是指由 X/Open 组织提出的分布式交易处理的规范。XA规范主要 定义了(全局)事务管理器(Transaction Manager)和(局部)资源管理器(Resource Manager)之间的接口。
note:
X/Open XA接口是双向的系统接口，在事务管理器以及一个或多个资源管理器之间形成通信桥梁。XA之所以需要引入事务管理器是因为，在分布式系统中，从理论上讲（参考FLP impossibility定理），两台机器理论上无法达到一致的状态，需要引入一个单点进行协调。事务管理器控制着全局事务，管理事务生命周期，并协调资源。资源管理器负责控制和管理实际资源。通常情况下，通过XA 接口规范，使用两阶段提交来完成一个全局事务，XA规范的基础是两阶段提交协议。
--
#### 2PC
- 二段提交（2PC）将提交分成两个阶段，第一阶段是准备阶段，由一个节点提议并收集其他节点的反馈，第二阶段是提交阶段，根据反馈情况决定是提交还是回滚。这里提议节点被称作协调者(coordinator)，其他参与节点被称为参与者(participants)。
--
#### 3PC
- 3PC是为解决两阶段提交协议的缺点而设计的。三阶段提交是“非阻塞”协议，在两阶段提交的第一阶段与第二阶段之间插入了一个准备阶段，使得原先在两阶段提交中，参与者在投票之后，由于协调者发生崩溃或错误，而导致参与者处于无法知晓是否提交或者中止的“不确定状态”所产生的可能相当长的延时的问题得以解决。
---
### 一致性模型
--
#### Strict Consistency（严格一致性模型）
- 所有的读操作都能读取到最新的修改，也就要求任何的写操作都可以立刻被同步到所有的节点中，在任一节点都可以读到写操作的内容，所以这里强调的是绝对的时钟上的即时同步，而实际的分布式系统中，任何信息的同步都需要有一定的延时，所以，strict consistency只能存在于理论，无法严格实现。
--
#### Sequential Consistency（顺序一致性）
-顺序一致性不要求绝对时钟，而是使用了逻辑时钟。任意一次执行的结果都像所有处理器的操作以某种顺序的次序执行所得到的一样，而且各处理器的操作都按照各自程序所指定的次序出现在这个顺序中。 
--
#### Linearizability Consistency（线性一致性）
- 线性一致性比顺序一致性要更严格。线性一致性在顺序一致性的约束基础上，额外增加了如下的约束：如果操作A开始时间晚于B结束时间，则在顺序一致性定序时，要求B在A前。直观上，线性一致性要求的是，操作生效时间是在操作发起到操作返回之间的某个时刻。线性一致性（也可以看做atomic consistency）可以当做是有实时性约束的顺序一致性。

- Cassandra就是利用Linearizability实现轻量级事务的，Google Spanner同样支持线性一致性。
--
#### Causal Consistency（因果一致性）
- 因果一致性是削弱了的线性一致性，它明确了只有存在因果关系的事件中需要保持线性一致性，允许因果关系的事件顺序可以对于整个分布式系统不一致。
--
#### Eventual Consistency（最终一致性）
- 最终一致性模型是用于分布式系统中，确保在如果一个更新操作发生，那么最终一定可以读取到这次的更新操作的发生。
- 相比于传统的ACID（Atomicity，Consistency，Isolation，Durability），最终一致性提供的是BASE（Basic Availablilty，Soft state，Eventual Consistency）服务。 
--
#### FIFO Consistency
- 意为管道式或流水线式存储器，因为一个进程的写能被管道化，故进程不需要等待每次写的完成就可启动下一个操作。要求同一个进程的程序顺序下的几个操作被其他所有进程看到的顺序是相同的。但是不同进程的几个操作被其他所有进程看到的顺序可能是不同的（即使有因果关系）。
--
#### Cache Consistency
- Cache Consistency要求所有进程涉及到的同一个地址的几个操作，要被所有进程看到一致的顺序。
--
####  Processor Consistency
- Processor Consistency是Cache Consistency + FIFO Consistency的结合，约束更多，因此这个一致性比另外两个都更强。
- Processor Consistency定义了在不同Processor可以是不同顺序，只需要所有其他Processor看来在同一Processor的写入操作必须是同一顺序的即可，所以这比因果一致性要弱。
--
#### Slow Consistency
- 如果进程读取先前写入内存位置的值，则无法在以后读取该位置的任何较早值。进程执行的写入在该进程中将立即可见，在其他进程不一定。Slow Consistency要比FIFO Consistency、Cache Consistency一致性要弱。
---
## 分布式系统的时间
- 分布式系统下需要记录和比较不同节点间事件发生的顺序，但不同于现实生活中使用物理时钟记录时间，分布式系统使用逻辑时钟记录事件顺序关系。为什么不适用物理时钟呢，这是由于现实生活中物理时间有统一的标准，而分布式系统中每个节点记录的时间并不一样，即使设置了 NTP 时间同步节点间也存在毫秒级别的偏差。并且存在网络的原因，一旦延时，可能会使事件乱序。
--
### Logic Clock （Lamport Clock）
- Leslie Lamport 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法。
--
### Vector Clock
- Vector clock可以解决并发公平性问题，它通过vector结构不但记录本节点的Lamport时间戳，同时也记录了其他节点的Lamport时间戳。
--
###  Version vectors
- Version vectors是Vector clock的一个变种，实现上与Vector clock类似，目的用于发现数据冲突。
--
### True Time
- NTP是有误差的，而且NTP还可能出现时间回退的情况，所以我们不能直接依赖NTP来确定一个事件发生的时间。在Google Spanner里面，通过引入True Time来解决了分布式时间问题。
- 全局序列号生成器虽然能达到类似目的，但它是整个系统的一个瓶颈，同时也避免不了网络开销，同时时间是一个非常直观的度量方式。
- Google 并没有论文说明如何构造 TrueTime，对于其他用户的实际并没有太多参考意义。
- TureTime 也会有误差范围，虽然非常的小，在毫秒级别以下，所以我们需要等待一个最大的误差时间，才能确保事务的相关顺序。
note:
虽然spanner引入了TrueTime可以得到全球范围的时序一致性，但相关事务在提交的时候会有一个wait时间ε（ε表示一个无限接近于0的一个无限小的正数），只是这个时间很短，而且spanner后续都准备将其优化到 ε < 1ms，也就是对于关联事务，仅仅在上一个事务commit之后等待2ms之后就能执行。
--
### Hybrid Logic Clock
- HLC是基于NTP的，但它只会读取当前系统时间，而不会去修改，同时HLC又能保证在NTP出现同步问题的时候仍能够很好的进行容错处理。对于一个HLC的时间t来时，它总是大于等于当前的系统时间，并且与其在一个很小的误差范围里面，也就是 |l - pt| < ε。
note:
HLC由两部分组成，physical clock + logic clock，l.j维护的是节点j当前已知的最大的物理时间，c.j则是当前的逻辑时间。那么判断两个事件的先后顺序就很容易了，先判断物理时间pt，在判断逻辑时间ct。

HLC虽然方便，它毕竟是基于NTP的，所以如果NTP出现了问题，可能导致HLC与当前系统pt的时间误差过大，其实已经不怎么精确了，HLC论文提到对于一些out of bounds的message可以直接忽略，然后加个log让人工后续处理，而cockroachdb是直接打印了一个warning log。
--
### TSO
- 如果整个系统不复杂，而且没有spanner那种跨全球的需求，一台中心授时服务就可以了。
- 在生产环境中，因为 TSO 的逻辑极其简单，同时 TSO 本身的高可用方案也容易实现。
---
## 分布式一致性协议与算法
* Cassandra和ElasticSearch、CockRoachDB使用了Gossip算法。
* Google的Chubby、Megastore（发表的论文里有关于mulit-paxos的公开细节）使用了Paxos
* zookeeper使用了Zab
* Raft是工程上应用最多的算法，例如etcd
---
### 选举、多数派、租约
* 选举（election）通过打破节点间的对等关系，选得的leader(或叫master、coordinator)也即2PC/3PC中的协调者，有助于实现事务原子性、提升决议效率。
* 多数派(quorum)帮助我们在网络分化的情况下达成决议一致性，在leader选举的场景下帮助我们选出唯一leader。
* 租约(lease)在一定期限内给予节点特定权利，也可以用于实现leader选举。
--
#### 选举
- 在分布式系统出现故障后，通常需要重新组织活动的节点使它们继续执行有用的任务。在这个重新组织和配置的过程中，第一步就是要选出一个协调者来管理这些操作。故障的检测通常是基于超时机制的。如果一个进程超过一定的时间没有收到协调者的响应，它就怀疑协调者出了故障并启动选举过程。选举在集群服务器、负载均衡、重复数据更新、应急恢复、连接组和互斥等领域都有广泛应用。一般来说，选举过程包括两步：一、选择一个具有最高优先级的leader，二、通知其他进程谁是leader。
--
#### 多数派
- 当分布式环境下出现网络分化时，由于出现了网络隔离，隔离之后会有多个节点都认为自己具有最大编号，将会产生多个协调者，所以引入了多数派的概念。多数派确保了在网络隔离情况下的leader的唯一性。假如节点总数为2X+1，则一项选举得到多于 X 节点赞成才能获得通过。leader选举中，网络分化场景下只有具备多数派节点的部分才可能选出leader，这避免了多leader的产生。因此，一般要保证分布式一致性，参与选举的节点数在集群中都要指定为单数个。
--
#### 租约
- 在网络拥堵或瞬断的情况下，leader状态可能不正常，但堵塞或瞬断结束后又正常，而这时，已经完成了选举，这容易导致出现双leader的情况。因此，引入了租约的概念。
- 每个租约时长内只有一个节点获得租约、到期后必须重新颁发租约。假设我们有一个租约颁发节点，节点在租约颁发节点上注册自己，租约颁发节点按选举颁发租约给节点，使节点成为leader，在租约期内，即使leader节点宕机，也不进行重新选举，到期后重新选举，颁发租约并确定leader。
- 在实际应用中，zookeeper、ectd均存在租约颁发。
---
## Gossip算法
- Gossip是一种去中心化、容错并保证最终一致性的协议。
- Gossip是一个最终一致性算法。
- Cassandra和ElasticSearch的自动发现节点机制都是使用了Gossip算法。
---
## Paxos 一致性算法
- Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。
- 节点通信存在两种模型：共享内存（Shared memory）和消息传递（Messages passing）。Paxos 算法就是一种基于消息传递模型的一致性算法。
--
### 在paxos算法中，分为4种角色：
1. Proposer ：提议者
2. Acceptor：决策者
3. Client：产生议题者
4. Learner：最终决策学习者
--
paxos协议的流程分为了两个阶段：
1. prepare阶段
2. accept阶段
--
### paxos算法中所有的行为：
1. Proposer提出议题
2. Acceptor初步接受 或者 Acceptor初步不接受
3. 如果上一步Acceptor初步接受则Proposer再次向Acceptor确认是否最终接受
4. Acceptor 最终接受 或者Acceptor 最终不接受
--
### 示例：两个参谋先后提议的场景
![paxos-1](images/paxos-1.png)
--
### 示例：两个参谋交叉提议的场景
![paxos-2](images/paxos-2.png)
---
## 分布式协调和配置服务
---
###  Zookeeper
- 基于ZAB协议，zookeeper实现了一种基于主备模式的系统架构来保证集群中各副本之间的数据一致性。
- ZAB协议可以细分为三个阶段：发现（discovery）、同步（sync）、广播(Broadcast)。
---
### Etcd 架构与实现 
1. raft通过对不同的场景（选主，日志复制）设计不同的机制，虽然降低了通用性（相对paxos），但同时也降低了复杂度，便于理解和实现。
2. raft内置的选主协议是给自己用的，用于选出主节点。
3. raft使用日志同步机制进行数据同步。

[Raft演示](http://thesecretlivesofdata.com/raft)
---
## 分区(Partitioning) 
- 原来所有的数据都是在一个数据库上的，网络IO及文件IO都集中在一个数据库上的，因此CPU、内存、文件IO、网络IO都可能会成为系统瓶颈。而分区的方案就是把某一个表或某几个相关的表的数据放在一个独立的数据库上，这样就可以把CPU、内存、文件IO、网络IO分解到多个机器中，从而提升系统处理能力。
---
## 分片(Replication) 
- 分区有两种模式，一种是主从模式，用于做读写分离；另外一种模式是分片模式，也就是说把一个表中的数据分解到多个节点中。
1. RANGE 分区：基于属于一个给定连续区间的列值，把多行分配给分区。 
2. LIST 分区：类似于按RANGE分区，区别在于LIST分区是基于列值匹配一个离散值集合中的某个值来进行选择。 
3. HASH分区：基于用户定义的表达式的返回值来进行选择的分区，该表达式使用将要插入到表中的这些行的列值进行计算。 
4. KEY 分区：类似于按HASH分区，区别在于KEY分区只支持计算一列或多列。 
---
## 一致性哈希(Consistent Hashing)
- 一致性哈希算法是分布式系统中常用的算法，是在1997年由麻省理工学院提出。在分布式集群中的每个节点仅需维护少量相邻节点的信息，并且在节点加入/退出系统时，仅有相关的少量节点参与到拓扑的维护中。所有这一切使得一致性哈希成为第一个实用的DHT（Distributed Hash Table，分布式哈希表）算法。
---
## 开源分布式数据库关键技术
--
### Spanner & TiDB中的事务
--
### Single Split Write
--
### Multi Split Write
--
### Strong Read
---
### 分布式数据库 Schema 设计
--
### 索引
--
### Spanner 的白皮书建议避免的情况 
--
### 查询优化
--- 
## 分布式数据库开发
--
## TiDB和CockroachDB为什么使用GO语言实现
--
## Impala和Kudu为什么使用C++语言实现
--
## BDRT为什么使用JAVA语言实现 
---
## 分布式数据库性能测试
---